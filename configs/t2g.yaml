model:
  target: modules.model.TransformerT2G
  params:
    base_learning_rate: 1.0e-5
    monitor: valid/loss 
    d_model: 512 
    nhead: 8
    dim_feedforward: 1024
    activation: relu
    n_layers: 4
    emb_dim: 512
    max_len: 100
    label_smoothing: 0.3
    text_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/train.text
    gloss_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/train.gloss
    text_vocab_cache: /home/ejhwang/plsl/cache/text_vocab.json
    gloss_vocab_cache: /home/ejhwang/plsl/cache/gloss_vocab.json
    vocab_freq: 1
data:
  target: modules.datamodule.DataModuleFromConfig
  params:
    batch_size: 32
    num_workers: 8
    train:
      target: modules.data.Phoenix2014T
      params:
        text_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/train.text
        gloss_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/train.gloss
        keypoint_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/train.skels
    validation:
      target: modules.data.Phoenix2014T
      params:
        text_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/dev.text
        gloss_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/dev.gloss
        keypoint_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/dev.skels 
    test:
      target: modules.data.Phoenix2014T
      params:
        text_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/test.text
        gloss_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/test.gloss
        keypoint_file_path: /data/sl_datasets/phoenix14t/data/ProgressiveTransformersSLP/test.skels 
lightning:
  callback:
    # keypoint_logger_callback:
    #   target: modules.callbacks.KeypointsLogger
    #   params:
    #     max_keypoints: 1
    #     save_and_sample_every: 1
    cuda_callback:
      target: modules.callbacks.CUDACallback
    # learning_rate_logger:
    #   target: main.LearningRateMonitor
    #   params:
    #     logging_interval: step 
  trainer:
    accumulate_grad_batch: 2
    accelerator: gpu
    devices: auto
    max_epochs: 100
    overfit_batches: 0.
    gradient_clip_val: 0.5