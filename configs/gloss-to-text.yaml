model:
  target: modules.models.GlossToText
  params: 
    lr: 2.0e-4
    monitor: valid/bleu4
    max_len: 128
    d_model: 768
    emb_dim: 512
    label_smoothing: 0.1
    nhead: 12
    dim_feedforward: 2048
    dropout: 0.1
    activation: relu
    n_layers: 3
    beam_size: 5
    text_corpus: /data/sl_datasets/phoenix14t/data/cache/phoenix_train_text_corpus.txt
    gloss_corpus: /data/sl_datasets/phoenix14t/data/cache/phoenix_train_gloss_corpus.txt
    min_freq: 1
    tokenizer_path: /data/sl_datasets/phoenix14t/data/cache/
    scheduler_config:
      target: modules.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps:
        - 300
        cycle_lengths:
        - 10000000000000
        f_start:
        - 1.0e-06
        f_max:
        - 1.0
        f_min:
        - 1.0
data:
  target: modules.datamodule.DataModuleFromConfig
  params:
    batch_size: 64
    num_workers: 32
    train:
      target: modules.lmdb.DatasetForLMBD
      params:
        db_path_list: 
        - /data/sl_datasets/phoenix14t/data/cache/phoenix_train.db
    validation:
      target: modules.lmdb.DatasetForLMBD
      params:
        db_path_list: 
        - /data/sl_datasets/phoenix14t/data/cache/phoenix_dev.db
    test:
      target: modules.lmdb.DatasetForLMBD
      params:
        db_path_list: 
        - /data/sl_datasets/phoenix14t/data/cache/phoenix_test.db
lightning:
  callback:
    # cuda_callback:
    #   target: modules.callbacks.CUDACallback
    learning_rate_logger:
      target: pytorch_lightning.callbacks.LearningRateMonitor
      params:
        logging_interval: step
  trainer:
    accumulate_grad_batch: 1
    accelerator: gpu
    devices:
    - 0
    max_epochs: 501
    overfit_batches: 0.0
    gradient_clip_val: 0.5
    precision: 16